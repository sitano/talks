Go Memory Model
Jule, 2018
Tags: go, memory, model

Ivan Prisyazhnyy
Software Engineer
@JohnKoepi

: talk: Good day!
: talk: How is everybody doing today?
: talk: ...
: talk: Cool!
: talk: ...
: talk: My name is Ivan and I am here today to talk to you about
: talk: Go memory model, and more specifically why do we need one,
: talk: how to understand it and how to apply it.
: talk:
: talk: You will also know what are data races, what kinds of memory models
: talk: exists, how Go MM is implemented, what are incorrectly synchronized
: talk: programs and how to detect data races.
: talk:
: talk: The talk consists of the three parts:
: talk: 1) First one introduces a memory models and answers the
: talk:    question WHY SHOULD YOU CARE ABOUT A MEMORY MODEL?
: talk:    Why do we need another level of abstraction on top of the
: talk:    hardware? Why compiler can't manage multithreading alone?
: talk:    What is correctly synchronized program?
: talk: 2) Second part retrospects parts of the memory models THEORY.
: talk:    You will learn a general way of how a memory models are defined,
: talk:    what they are consists of, how the model reflects the program and its
: talk:    states. It's about a happens-before, consistency and DRF.
: talk: 3) In the third one you will get a glimpse of the details
: talk:    of the GO MEMORY MODEL. What it is based on? What it
: talk:    describes and how. Is it a full spec or not? How atomics
: talk:    works? There are will be an examples of the data races and
: talk:    under synchronized programs.
: talk: 4) Part four will tell a few words on the RACE DETECTION and
: talk:    `tsan` implementation.

* Opinions are my own

Anything on this or any subsequent slides may be a lie. Do
not base your decisions on this talk.

.link https://golang.org/ref/mem#tmp_1 don't be clever advice

: talk: Anything on this or any subsequent slides may be a lie. Do
: talk: not base your decisions on this talk.

* What is a memory model?

: talk: What is a memory model and why do we need one?

* How to answer a question:

Uniprocessors : we are fine - programs are sequential.
Multicores : different types of parallelism we must handle: task, mem, inst.

What is a correct program?

.image mm/whaaaat.jpg

: talk: There is no questions when we are writing single threaded programs.
: talk: But let's assume we are writing programs that exploit parallelism.
: talk: There are 3 different kinds of parallelism: task, memory, and instructions.
: talk: Programmers are given different synchronization and orderding
: talk: constructions to make use of available hardware parallelism. But
: talk: correct usage is hard. It's easy to miss something even in some very
: talk: obvious parts. People tend to under synchronize or over synchronize
: talk: their code that leads to errors, invalid results, counterintuitive
: talk: behavior or excessive performance overheads. So it's important
: talk: to use weakest ordering modes possible correctly.

: talk: How to understand whether the program is correctly synchronized?
: talk: Easy answer - it does not produce invalid behaviors ever.

: talk: Try to guess, are the following programs correctly synchronized?

* 1. Why should we care?

.code mm/print_ab.go /START OMIT/,/END OMIT/

- Is that correct program?
- What are possible outcomes?

: talk: Is that program correct? What are possible results of this program?
: talk: How to answer such a questions anyway?

* 1+. Why should we care?

.code mm/print_ab.go /START OMIT/,/END OMIT/

- Is that correct program?
- What are possible outcomes?
- Is (2, 0) a possible outcome?

: talk: While answering previous questions, you also have to answer i.e. is
: talk: (2, 0) outcome is possible. Is it possible? How to understand it?

* 2. Why should we care?

.code mm/wait_idiom.go /START OMIT/,/END OMIT/

- Is that correct program?
- What are possible outcomes?

: talk: What about this one? Is that program correct? What are possible
: talk: results of this program?

: talk: So, remember your answers. Now, let's move on to the question:
: talk: How to understand whether a multithread program is correctly synchronized?

* Race Condition is not we are looking for

"A race condition is the behavior of the system where the output
is dependent on the sequence or timing of other uncontrollable events.
It becomes a bug when events do not happen in the order the programmer intended."

.link https://en.wikipedia.org/wiki/Race_condition Wikipedia (def.)

: some kind of external timing or ordering non-determinism

: talk: There is a notion of race condition, which is defined as follows.

* Data Race

"a race condition caused by potentially concurrent operations on a shared memory location, of which at least one is a write."

.link https://en.wikipedia.org/wiki/Race_condition#Software Wikipedia/Software/C++ x11

: talk: And there is a notion of data race.

* Race Conditions != Data Races,

It's not just race conditions are about _events_, and the data races are about the events of memory  _accesses_.

.link https://blog.regehr.org/archives/490 Race Condition vs. Data Race

Data Races are *bad*.

.link https://stackoverflow.com/questions/11276259/are-data-races-and-race-condition-actually-the-same-thing-in-context-of-conc

: talk: The thing is the data races are bad, while race conditions are not
: talk: always bad. They are confused sometimes, so don't be.
: talk: The correctly synchronized program is data race free.

* Example of data race and race condition

    var count = 0                       var count = 0
                                        var m = sync.Mutex{}
    func incrementCount() {             func set(x int) {
        if count == 0 {                     m.Lock()
            count ++                        count = x
        }                         vs        m.Unlock()
    }                                   }

    func main() {                       func main() {
        go incrementCount()                 go set(1)
        go incrementCount()                 go set(2)
    }                                   }

- What are possible outcomes?
- Where is a DR, where is a RC?
- "A program must be correctly synchronized to avoid the kinds of counterintuitive behaviors that can be observed when code is reordered",

: talk: Take a look at following code examples. How do you think, where
: talk: it is a DR and where it is a RC? Are they equally harmful?

: talk: Both are capable of producing 2 or 1. But are they equally harmful?

* Data Race Free (DRF) programs imposes SC

1. "A program must be correctly synchronized to avoid the kinds of counterintuitive behaviors that can be observed when code is reordered",
2. "If a program is correctly synchronized, then all executions of the program will appear to be sequentially consistent".
3. "A program is correctly synchronized if and only if all sequentially consistent executions are free of data races",

: talk: Knowing data races are bad we can conclude a notion of a correctly
: talk: synchronized program. Simply put, its a program that is free of data races.
: talk: Then all program sequentially consistent executions are free of data races.

.link https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.4 JMM

* How can we understand that the program is correct?

Just make it race-free.

- In order of doing so, you must made it correct under a memory model.
- Memory models specifies which executions of the program are correct.

: talk: So now we've got an idea of what is a correctly synchronized program.
: talk: Just bear memory model in mind when writing programs. It sounds so easy.

* Memory Model loves you

So we are just writing correctly synchronized programs? Sounds so easy.

Memory model:

- which programs are correct
- what those programs can do
- therefore what programmers can expect
- therefore what compiler writers must ensure / can do

We want weaker consistency => more potential parallelism.

: talk: A memory model is a model which describes semantics of the memory of the
: talk: abstract machine by defining various memory ordering modes, synchronization
: talk: constructs and consistency rules. It allows to effectively abstract a program
: talk: from the hardware implementation to exploit its parallelism capabilities and
: talk: in the meantime to reason about program correctness and behaviors. It allows
: talk: writing data race free multithreaded programs.

: talk: It can answer question: what executions of the program are valid and
: talk: defines some notion of correctly synchronized program. Going from that
: talk: compilers are free to perform optimizations to the programs that obey SM.

: talk: But why do we need another level of abstraction? Why other layers that exists
: talk: (like hardware or compiler) in-between can't handle the problem them selves?
: talk: Let's take a look how different kinds of parallelism affects program behaviors.

* History intro

* A Fairy Tale

In a past good days everything was simple.

Once upon a time…

> How do you make a program faster?

    < Buy newer hardware.

> Is a hardware or compiler optimization valid?

    < Yes, if valid programs don’t change behavior.
    (And most programs are valid.)

.html mm/source_dave.html

: talk: In a past good days everything was simple.
: talk: Uniprocessors single-stepped through instructions accessing memory cells.
: talk: Parallelism emulated with context-switching. It was era of simple compilers,
: talk: simple conveyors, simple caches: new hardware is valid if it does not change behavior.

* An Evil Twist

Hardware engineer’s magic spells stopped working.

    > New magic spell: stamp out more and more cores.

Compiler and operating system engineers gave us multithreaded programming.

Now hardware, compiler optimizations change program behaviors.

- multiple issue of instructions
- out-of-order execution
- speculative execution
- optimizations on various levels
- multiple core, caches

.html mm/source_dave.html

: talk: At one moment, everything changed. Hardware guys started to
: talk: build multi cores systems. Compilers and OSes became complicated
: talk: software performing complex optimizations. Multithreaded
: talk: programming appeared. And since then the new paradigm was
: talk: the hardware and compilers change program's behavior.

* How can we get along?

“Valid optimizations do not change the behavior of valid programs.”

: talk: So, how can we get along all this new fancy stuff?

back to our code example:

.code mm/wait_idiom.go /START OMIT/,/END OMIT/

Can it ever print 0?

.html mm/source_dave.html

: talk: Back to our code example, can this program ever print 0?

* Can it ever print 0?

“It depends.”

: talk: And the answer is - it depends.

.code mm/wait_idiom.go /START OMIT/,/END OMIT/

In x86 assembly, *no*.
In ARM/POWER assembly, *yes*.
In most C compilers (even on x86), *yes* (or maybe it won’t even finish).

: talk: Depending on the hardware architecture and compiler the answer
: talk: varies.

.html mm/source_dave.html

* “It depends” is not a happy ending

    Hardware: x86, arm, ...
    ---------------------------------\
      Abstract Machine: ir, jvm      |
      Compiler: gcc, llvm, ic, jvm   | <- Memory Model of our abstract machine
      Language: asm, c, java, go     |    is somewhere there...
    ---------------------------------/
    Program

.html mm/source_dave.html

: talk: And the answer "it depends" is not a happy ending.

: talk: Before getting to the language memory model we need to
: talk: look at the hardware level memory models and understand
: talk: what guarantees it provides.

* Hardware Memory Models

: Memory ordering describes the order of accesses to computer memory by a CPU. The term can refer either to the memory ordering generated by the compiler during compile time, or to the memory ordering generated by a CPU during runtime. Runtime memory ordering. In symmetric multiprocessing (SMP) microprocessor systems.

: talk: Let's imagine we are writing with an assembly language.
: talk: How badly can the hardware mess with us?

: talk: Let's start with a strong consistency guarantees - sequential consistency.

* 1. Sequential Consistency

> all reads and all writes are in-order

    “The customary approach to designing and proving the correctness of
    multiprocess algorithms for such a computer assumes that the following condition
    is satisfied: the result of any execution is the same as if the operations of all the
    processors were executed in some sequential order, and the operations of each
    individual processor appear in this sequence in the order specified by its program.
    A multiprocessor satisfying this condition will be called sequentially consistent.”

— Lamport, “How to Make a Multiprocessor Computer That Correctly Executes
Multiprocess Programs” (1979)

: talk: One might expect multiprocessors to have sequentially consistent (SC) shared memory.

: talk: Sequential consistency is a very strong guarantee that is made about visibility and ordering in an execution of a program. Within a sequentially consistent execution, there is a total order over all individual actions (such as reads and writes) which is consistent with the order of the program, and each individual action is atomic and is immediately visible to every thread. or in other words all RWS are in-order.

: talk: Because stronger modes impose more ordering constraints, they reduce potential parallelism, in at least one of the above senses -- if operations are performed in parallel, ordering may require that one activity block (reducing total parallelism and adding overhead) waiting for completion of another. Ordering may also provide guarantees that programs rely on.

: talk: The Seq. Con. is our baseline of the strong consistency for the memory models.

* 1. Sequentially Consistent Hardware

.image mm/hw_sc.png

Maranget et al.,
.link https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf “A Tutorial Introduction to the ARM and POWER Relaxed Memory Models”

: talk: This is how this architecture looks like. It does not do local reordering and all
: talk: writes are immediately visible to consequent reads.

* Litmus Test: Message Passing

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  r1 = y
      y = 1   |  r2 = x

Can this program see r1 = 1, r2 = 0?

.html mm/source_dave.html

: talk: Can this program see r1 = 1, r2 = 0 under seq. con.?

* Message Passing w/ Sequential Consistency

.html mm/sce_msg_pass.html

Can this program see r1 = 1, r2 = 0?

On sequentially consistent hardware: *no*.

.html mm/source_dave.html

: talk: Applying this definition to our example the answer is NO for seq. con. architecture.
: talk: On seq. con. arch. it can't reorder events in the same thread. So the writes from t1
: talk: can't be observed in different order by t2, because execution in which it happens to
: talk: be [... y=1 ... x=1 ...] is not consistent to the PO.

* 2. Relaxed ordering: x86 Hardware (Total Store Order)

.image mm/hw_sto.png _ 650

Maranget et al.,
.link https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf “A Tutorial Introduction to the ARM and POWER Relaxed Memory Models”

: talk: Now take a look at how x86 architecture looks like. Do you see a difference? It has a
: talk: store buffers (FIFO write buffer of pending memory writes), and global write lock
: talk: which provides TSO architecture type. It has more relaxed memory ordering semantics,
: talk: although some types of reordering are still forbidden.

* Short note on TSO architecture

Relaxed consistency (some types of reordering are allowed)

- Loads can be reordered after loads (for better working of cache coherency, better scaling): no
- Loads can be reordered after stores: no
- Stores can be reordered after stores: no
- Stores can be reordered after loads: yes

.link https://en.wikipedia.org/wiki/Memory_ordering

: talk: The difference of TSO architecture which is a more relaxed memory ordering than seq. con.
: talk: is that stores can be reordered after loads.

* Litmus Test: Message Passing

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  r1 = y
      y = 1   |  r2 = x

Can this program see r1 = 1, r2 = 0?
On sequentially consistent hw: no.
On x86 (or other TSO): *no*.

    Thread 1’s writes are observed by other threads in original order.

.html mm/source_dave.html

: talk: Can this program see r1 = 1, r2 = 0?
: talk: No, thread 1’s writes are observed by other threads in original order,
: talk: because writes are not reordered.

* Litmus Test: Store Buffering

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  y = 1
      r1 = y  |  r2 = x

Can this program see r1 = 0, r2 = 0?
On sequentially consistent hw: *no*.
On x86 (or other TSO): *yes*!

    Thread 1’s local writes are not immediately visible in Thread 2 (and vice versa).

: talk: Can this program see r1 = 0, r2 = 0?
: talk: On seq. con. hw. no, and on x86 yes, thread 1’s local writes are not
: talk: immediately visible in thread 2 (and vice versa). Stores can be reordered
: talk: after loads like this [ read x, read y, write x, write y, write r1, write r2  ].

.html mm/source_dave.html

* Memory Fences

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  y = 1
      *fence* |  *fence*
      r1 = y  |  r2 = x

Can this program see r1 = 1, r2 = 0? No.

Memory fence ensures Thread 1’s write is globally visible before Thread 2’s read,
and vice versa.

.html mm/source_dave.html

: talk: To help to solve code synchronization problems there are memory fences.
: talk: Memory fences are a type of barrier instructions that causes a CPU or compiler
: talk: to enforce an ordering constraint on memory operations issued before and after
: talk: the barrier instruction.

: talk: Can this program see r1 = 1, r2 = 0? No. Memory fence ensures thread 1’s
: talk: write is globally visible before Thread 2’s read, and vice versa.

* Litmus Test: Independent Reads of Indep. Writes

    Thread 1 | T2    | T3     | T4
    ---------------------------------
    x = 1    | y = 1 | r1 = x | r3 = y
                       r2 = y | r4 = x

Can this program see r1 = 1, r2 = 0, r3 = 1, r4 = 0?

Can Thread 3 see x change before y but Thread 4 see the opposite?

On sequentially consistent hw: no.
On x86 (or other TSO): no.

There is a total order over all stores to main memory.

.html mm/source_dave.html

: talk: Another litmus test on total order of all store in main memory.
: talk: Can Thread 3 see x change before y but Thread 4 see the opposite?
: talk: The answer is no on totally ordered writes architectures.

* 3. Weak ordering: ARM/POWER/Alpha Hardware

.image mm/hw_arm.png _ 450

Maranget et al.,
.link https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf “A Tutorial Introduction to the ARM and POWER Relaxed Memory Models”

: talk: Now let's take a look at ARM architecture. ARM and IBM POWER have adopted
: talk: considerably more relaxed memory models.

: talk: ARM is not a TSO. It can arbitrary reorder reads and writes or even
: talk: execute them speculatively. Memory model does not guarantee that a
: talk: write becomes visible to all other hardware threads at the same time point;
: talk: these architectures are not multiple-copy atomic.

: talk: This plot shows an effective way of thinking of such architecture as
: talk: a separate cores having an independent memories agents which can propagate
: talk: writes to the other threads so those writes may interleave arbitrary unless
: talk: specified by the fences and coherence.

* Litmus Test: Message Passing

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  r1 = y
      y = 1   |  r2 = x

Can this program see r1 = 1, r2 = 0?
On sequentially consistent hw: no.
On x86 (or other TSO): no.
On ARM/POWER: *yes*!

    Thread 1’s writes may not be observed by other threads in original order.

.html mm/source_dave.html

: talk: Back to our litmus test 1. Can this program see r1 = 1, r2 = 0?
: talk: Yes for the ARM architecture. Thread 1’s writes may not be observed
: talk: by other threads in original order.

* Litmus Test: Store Buffering

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  y = 1
      r1 = y  |  r2 = x

Can this program see r1 = 0, r2 = 0?
On sequentially consistent hw: no.
On x86 (or other TSO): yes!
On ARM/POWER: *yes*!

    Thread 1’s local writes are not immediately visible in Thread 2 (and vice versa).

.html mm/source_dave.html

: talk: Litmus test 2. Can this program see r1 = 0, r2 = 0? It's yes for ARM.
: talk: Thread 1’s local writes are not immediately visible in Thread 2 (and vice versa).

* Litmus Test: Independent Reads of Indep. Writes

    Thread 1 | T2    | T3     | T4
    ---------------------------------
    x = 1    | y = 1 | r1 = x | r3 = y
                       r2 = y | r4 = x

Can this program see r1 = 1, r2 = 0, r3 = 1, r4 = 0?

(Can Thread 3 see x change before y but Thread 4 see the opposite?)

On sequentially consistent hw: no.
On x86 (or other TSO): no.
On ARM/POWER: *yes*!

    Different threads may receive different writes in different orders.

.html mm/source_dave.html

: talk: Litmus test 3. Can Thread 3 see x change before y but Thread 4 see the opposite?
: talk: Yes on ARM. Different threads may receive different writes in different orders.

* Litmus Test: Coherence

    Thread 1 | T2    | T3     | T4
    ---------------------------------
    x = 1    | x = 2 | r1 = x | r3 = x
                       r2 = x | r4 = x

Can this program see r1 = 1, r2 = 2, r3 = 2, r4 = 1?

(Can Thread 3 see x=1 before x=2 but Thread 4 see the opposite?)

On sequentially consistent hw: no.
On x86 (or other TSO): no.
On ARM/POWER: *yes*!

    Different threads may receive different writes in different orders.

.html mm/source_dave.html

: talk: And thats an example of how coherence properties are different for these
: talk: architectures.

* Weak Ordering Memory Model

“Let a synchronization model be a set of constraints on memory accesses that
specify how and when synchronization needs to be done.

Hardware is weakly ordered with respect to a synchronization model if and only if
it appears sequentially consistent to all software that obey the synchronization
model.”

.html mm/source_dave.html

.link http://pages.cs.wisc.edu/~markhill/papers/isca90_drf0.pdf Adve and Hill, “Weak Ordering - A New Definition” (1990)

: talk: In 1990 a new definition of the weak ordering was given by the Adve and Hill.
: talk: Since then modern processor architecture uses various types of weak ordering.
: talk: And correctly synchronized programs appears to be SC.

: talk: “Let a synchronization model be a set of constraints on memory accesses that specify how and when synchronization needs to be done. Hardware is weakly ordered with respect to a synchronization model if and only if it appears sequentially consistent to all software that obey the synchronization model.”

: Old definition, more constrained (Weak Consistency): In a multiprocessor system, storage accesses are weakly ordered if (1) accesses to global synchronizing variables are strongly ordered, (2) no access to a synchronizing variable is issued by a processor before all previous global data accesses have been globally performed, and if (3) no access to global data is issued by a processor before a previous access to a synchronizing variable has been globally performed.

* Synchronization model: Data-Race-Free (DRF0)

Synchronization operations are (to hardware) recognizably different from ordinary
operations.

A program is data-race-free if for all idealized SC executions, any two ordinary
memory accesses to the same location from different threads are either:

- both reads
- or separated by synchronization operations: one _happens-before_ the other

.html mm/source_dave.html

"We believe that DRF allows for faster hardware without significantly reducing software flexibility."

.link http://pages.cs.wisc.edu/~markhill/papers/isca90_drf0.pdf Adve and Hill, “Weak Ordering - A New Definition” (1990)

: talk: Now we can define a DRF0 synchronization model to be obeyed by the software.
: talk: A program is data-race-free if for all idealized SC executions, any two ordinary
: talk: memory accesses to the same location from different threads are either both reads, or
: talk: separated by synchronization operations.

* Data-Race-Free

.image mm/drf_free.png _ 600

.link http://pages.cs.wisc.edu/~markhill/papers/isca90_drf0.pdf Adve and Hill, “Weak Ordering - A New Definition” (1990)

: talk: Example 1 of DRF program.

* Not Data-Race-Free

.image mm/drf_non_free.png _ 550

.link http://pages.cs.wisc.edu/~markhill/papers/isca90_drf0.pdf Adve and Hill, “Weak Ordering - A New Definition” (1990)

: talk: Example 2 of DRF program.

* Hardware weakly ordered by DRF

“Hardware is weakly ordered with respect to [DRF] if and only if it appears
sequentially consistent to all software that obey [DRF].”

.html mm/source_dave.html

.link http://pages.cs.wisc.edu/~markhill/papers/isca90_drf0.pdf Adve and Hill, “Weak Ordering - A New Definition” (1990)
.link https://herbsutter.com/2012/08/02/strong-and-weak-hardware-memory-models/

: talk: Now applying the weak ordering definition to the DRF0 synchronization model we
: talk: are getting: Hardware is weakly ordered with respect to [DRF] if and only if it appears
: talk: sequentially consistent to all software that obey [DRF]. Which means...

* Hardware weakly ordered by DRF when...

    - Intra-processor dependencies are preserved
    - All writes to same location have a global total order (coherence)
    - All sync operations to same location have a global total order
        - for S1 before S2, all of S1 must complete before any of S2 starts.
    - A new access is not generated by a processor until previous sync operations
    are committed.
    - Once a sync operation S by processor P is committed, no other sync
    operations on the same location can commit until:
        - all reads by P before S must be committed
        - all writes by P before S must be globally performed

.html mm/source_dave.html

: talk: that intra-threads dependencies are preserved, coherence is preserved,
: talk: sync ops are recognized, respected and are totally ordered.

* Hardware weakly ordered by DRF

Basically everything, given appropriate synchronization implementations

- Earlier hardware models
- VAX
- x86
- ARM/POWER

guarantee that DRF implies appearance of SC: DRF-SC.

.html mm/source_dave.html

: talk: Basically everything, given appropriate synchronization implementations
: talk: guarantee that DRF implies appearance of SC: DRF-SC.

* Compilers

Significant freedom to rewrite code.
Significant gaps in knowledge of execution.

    w = 1
    x = 2
    r1 = y
    r2 = z

Compiled code fails (answers *yes!* to) every litmus test we’ve seen,
including coherence!

.html mm/source_dave.html

: talk: Now, let's move on to the compilers. Compilers are the next big thing which
: talk: can change programs behavior. They do that according to that abstract machine
: talk: they are working upon.

: talk: Compilers have significant freedom to rewrite code and they have
: talk: significant gaps in knowledge of execution. Compiled code fails
: talk: (answers *yes!* to) every litmus test we’ve seen, including coherence!

* Compiler Optimizations

> Compiler always expects the programs are DRF

: talk: Compiler always expects the programs are DRF.

* Compiler Optimizations

Is this a valid optimization?

.html mm/compiler_opt_1.html

Compiler and language must be involved in multithreaded guarantees.

.link http://www.hpl.hp.com/techreports/2004/HPL-2004-209.pdf Boehm, “Threads Cannot be Implemented as a Library” (2004):

.html mm/source_dave.html

: talk: This is an example of questioning optimization bounds by the compiler.
: talk: Is this a valid optimization?

: talk: The main idea in the end is that compiler and language must be involved
: talk: in multithreaded guarantees.

* Weak Ordering?

“Hardware is weakly ordered with respect to a synchronization model if and only if
it appears sequentially consistent to all software that obey the synchronization
model.”

> Why not a programming language (implementation)?

.html mm/source_dave.html

: talk: And as a result the ideas on weak ordering with respect to the DRF-SC
: talk: on a hardware can be transposed to the programming language / compiler.

: talk: Now, let's take a look on how a memory model for an language abstract
: talk: machine can be defined.

* Theory

Memory model:

- defines which executions of a program are legal
- thus, allows to understand which optimizations are correct
- defines memory accesses ordering modes
- defines ordering relations: when represented as graphs with events as nodes and relations as links, total orders form linear lists, and partial orders form DAGs -- directed acyclic graphs.

"A program is correctly synchronized if and only if all sequentially consistent executions are free of data races."

"If a program is correctly synchronized, then all executions of the program will appear to be sequentially consistent (§17.4.3)."

.link https://en.wikipedia.org/wiki/Memory_model_(programming)
.link https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.4

: talk: 1) A memory model describes possible behaviors of a program.
: talk: 2) An implementation is free to produce any code it likes, as long as
: talk: all resulting executions of a program produce a result that can be
: talk: predicted by the memory model.
: talk: 3) in other words: the memory model determines what values can be read
: talk: at every point in the program.

: talk: How it is doing so?

: talk: A memory model defines which execution traces of a program are legal
: talk: executions of a program. We need a program to be correctly synchronized
: talk: with respect to the memory model to ensure only expected behaviors
: talk: (and results).

: talk: A program is correctly synchronized if and only if all sequentially
: talk: consistent executions are free of data races. And it works vice versa.

: talk: But why we are just not using seq. con. as our memory model?

: talk: Sequential consistency is a very strong guarantee that is made about
: talk: visibility and ordering in an execution of a program. If we were to use
: talk: sequential consistency as our memory model, many of the compiler and
: talk: processor optimizations that we have discussed would be illegal.

* Theory

- Program => Executions => Outcomes,
- `Program` consists of inter/intra-thread `Actions`,
- `Executions` are possible histories of committed `Actions`,
- `Behaviours` may be incorrect and counterintuitive if something wrong.

Executions are the behaviors of the abstract machine (AM), not
the behavior of final implementation. They define all possible
ways the program can possibly execute.

: talk: So, we have a program to be executed. A program consists of actions which
: talk: can be inter or intra thread. Executions define all possible ways the program can
: talk: possibly execute. Executions consists of execution traces of program actions or
: talk: histories. Executions are the behaviors of the abstract machine (AM), not the behavior
: talk: of final implementation. Executions result in an outcomes.

: talk: Then the implementation may produce a subset of those executions and the
: talk: results are the subset of the AM executions outcomes.

    Formaly, an execution E is described by a tuple < P, A, po, so, W, V, sw, hb > (JMM)

: P - a program
: A - a set of actions
: po - program order, which for each thread t, is a total order over all actions performed by t in A
: so - synchronization order, which is a total order over all synchronization actions in A
: W - a write-seen function, which for each read r in A, gives W(r), the write action seen by r in E.
: V - a value-written function, which for each write w in A, gives V(w), the value written by w in E.
: sw - synchronizes-with, a partial order over synchronization actions
: hb - happens-before, a partial order over actions

: Note that the synchronizes-with and happens-before elements are uniquely determined by the other components of an execution and the rules for well-formed executions (§17.4.7).
: An execution is happens-before consistent if its set of actions is happens-before consistent (§17.4.5).

: Well-Formed Executions
: We only consider well-formed executions. An execution E = < P, A, po, so, W, V, sw, hb > is well formed if the following are true:
: 1. Each read sees a write to the same variable in the execution.
:    All reads and writes of volatile variables are volatile actions. For all reads r in A, we have W(r) in A and W(r).v = r.v. The variable r.v is volatile if and only if r is a volatile read, and the variable w.v is volatile if and only if w is a volatile write.
: 2. The happens-before order is a partial order.
:    The happens-before order is given by the transitive closure of synchronizes-with edges and program order. It must be a valid partial order: reflexive, transitive and antisymmetric.
: 3. The execution obeys intra-thread consistency.
:    For each thread t, the actions performed by t in A are the same as would be generated by that thread in program-order in isolation, with each write w writing the value V(w), given that each read r sees the value V(W(r)). Values seen by each read are determined by the memory model. The program order given must reflect the program order in which the actions would be performed according to the intra-thread semantics of P.
: 4. The execution is happens-before consistent (§17.4.6).
: 5. The execution obeys synchronization-order consistency.
:    For all volatile reads r in A, it is not the case that either so(r, W(r)) or that there exists a write w in A such that w.v = r.v and so(W(r), w) and so(w, r).

: + Causality requirements $17.4.8

* Theory

Executions ≈ Actions ∪ Orders ∪ Consistency Rules

Actions:

- Normal store, load (plain mode)
- Synchronization actions: volatile store load (seq_cst mode), acq/rel mode actions, lock/unlock (mutual exclusion), synthetic (first action of the goroutine), actions that start threads/goroutines or destroys thema
- External actions

: External Actions. An external action is an action that may be observable outside of an execution, and has a result based on an environment external to the execution.

- Divergent actions

.html mm/source_alex.html

: Thread divergence actions (§17.4.9). A thread divergence action is only performed by a thread that is in an infinite loop in which no memory, synchronization, or external actions are performed. If a thread performs a thread divergence action, it will be followed by an infinite number of thread divergence actions.

: Thread divergence actions are introduced to model how a thread may cause all other threads to stall and fail to make progress.

: talk: All possible executions are defined by a union of the program actions,
: talk: orders and consistency rules. There are also various additional
: talk: constraints like causality, coherence, termination, intra-thread semantics, etc.
: talk: for executions to be well-formed.

: talk: Actions are constituted of intra and inter thread actions. Usually, spec.
: talk: requires threads to obey intra-thread semantics. Inter thread actions
: talk: performed by one thread that can be detected or directly influenced by
: talk: another thread.

: talk: Inter thread actions consists of normal memory accesses and synchronization
: talk: actions. Synchronization actions are special types of actions which allows
: talk: defining -various-types-of-memory-barriers- ordering of the other actions.

* Theory

Executions ≈ Actions ∪ Orders ∪ Consistency Rules

Orders:

- Program Order (PO): intra-thread semantics

: Among all the inter-thread actions performed by each thread t, the program order of t is a total order (execution order) that reflects the order in which these actions would be performed according to the intra-thread semantics of t.

: talk: First order to be introduced is a Program Order (PO). Among all
: talk: the inter-thread actions performed by each thread t, the program
: talk: order of t is a total order (execution order) that reflects the
: talk: order in which these actions would be performed according to the
: talk: intra-thread semantics of t.

: talk: In other words PO is an order which is consistent with a program
: talk: intra-threads semantics.

- Synchronization Order (SO): SA total order + SW relation

: A synchronization order is a total order over all of the synchronization actions of an execution. For each thread t, the synchronization order of the synchronization actions (§17.4.2) in t is consistent with the program order (§17.4.3) of t.

: talk: Next, a synchronization order is a total order over all of the synchronization
: talk: actions of an execution. For each thread t, the synchronization order of
: talk: the synchronization actions in t is consistent with the program order of t.

: talk: Synchronization actions induce the synchronized-with relation on actions.
: talk: The source of a synchronizes-with edge is called a release, and the destination is called an acquire.

- Happens-Before (HB) order

: If we have two actions x and y, we write hb(x, y) to indicate that x happens-before y.
: If x and y are actions of the same thread and x comes before y in program order, then hb(x, y).
: There is a happens-before edge from the end of a constructor of an object to the start of a finalizer (§12.6) for that object.
: If an action x synchronizes-with a following action y, then we also have hb(x, y).
: If hb(x, y) and hb(y, z), then hb(x, z).

"A set of synchronization edges, S, is sufficient if it is the minimal set such that the transitive closure of S with the program order determines all of the happens-before edges in the execution. This set is unique."

: talk: Two actions can be ordered by a happens-before relationship.
: talk: If one action happens-before another, then the first is visible to and ordered before the second.
: talk: HB is a partial order and a transitive closure over a union of PO and SO.
: talk: A set of synchronization edges, is sufficient if it is the minimal set such
: talk: that the transitive closure of S with the program order determines all
: talk: of the happens-before edges in the execution. This set is unique.

: talk: It should be noted that the presence of a happens-before relationship between
: talk: two actions does not necessarily imply that they have to take place in that
: talk: order in an implementation. If the reordering produces results consistent
: talk: with a legal execution, it is not illegal. (according to the def. of weak ordering con.)

: talk: More specifically, if two actions share a happens-before relationship,
: talk: they do not necessarily have to appear to have happened in that order
: talk: to any code with which they do not share a happens-before relationship.
: talk: Writes in one thread that are in a data race with reads in another thread may,
: talk: for example, appear to occur out of order to those reads.

: talk: The happens-before relation defines when data races take place.

.link https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.4 JMM ($17.4.5)

.html mm/source_alex.html

* Theory

Executions ≈ Actions ∪ Orders ∪ Consistency Rules

Consistency Rules:

    > A consistency rule that affects values observed by the actions.

- PO consistency - PO is consistent with the source code order in the original program.

: PO consistency affects the structure of the execution.

: talk: PO is a total order for any given thread in isolation. PO consistency is
: talk: that PO order is consistent with the source code order in the original program.
: talk: PO consistency affects the structure of the execution.

- SO consistency - reads see only latest writes in SO

: talk: SO is a total order over all synchronization actions. SO consistency is
: talk: that SO reads see only the latest writes in SO. SO is a consistency rule
: talk: that affects values observed by the actions.

- SO - PO consistency - SO and PO agree.

: talk: SO - PO consistency is about SO and PO agreement. PO+SO+SO-PO gives us a coherence.

: Coherence (def.) : The writes to the single memory location appear to be in a total order consistent with program order

- HB consistency - HB is a transitive closure over the union of PO and SW. HB is partial order too.

: talk: HB is a transitive closure over the union of PO and SW. HB is partial order too.
: talk: HB consistency is that reads in HB order observe either the last write in hb order,
: talk: or any other write, not ordered by hb.

: talk: HB + additional requirements provides causality (release - acquire semantics).

: see JMM: Example 17.4.8-1. Happens-before Consistency Is Not Sufficient

: talk: Mutual exclusion accesses guarded with mutexes provides sequential consistency.

.html mm/source_alex.html

* Happens-Before and C4

- Happens-before was defined to specify whether a program has a race.
- Can it alone be used to specify the possible behaviors of a program? Not quite. HB alone does not provide coherence or causality.

Adve and Boehm, “Memory Models: A Case for Rethinking Parallel Languages and Hardware” (2010)

Batty et al, “The Problem of Programming Language Concurrency Semantics” (2015)

.html mm/source_dave.html

* C4 (Commutativity, Coherence, Causality, Consensus)

- are required
- some set with additional constraints
- from which different ordering mode arises of different guarantees

rule of thumb: usually at least a causality is required to be maintained by the
synchronization components to be usable for inter-thread communication.

.link http://gee.cs.oswego.edu/dl/html/j9mm.html#summarysec J9MM by Doug Lea

: Opaque mode supports designs relying on coherence: the guarantee that visible overwrites to each variable are ordered, along with associated guarantees about access atomicity and progress, ensures awareness of variables across threads.
: Release/Acquire mode additionally supports designs relying on causality: strict partial ordering of the antecedence relation enables communication across threads.
: Volatile mode additionally supports designs relying on consensus: total ordering of accesses enables threads to reach agreement about program states.

* Theory

- A program is correctly synchronized if and only if all sequentially consistent executions are free of data races.
- If a program is correctly synchronized, then all executions of the program will appear to be sequentially consistent

: talk: And finally all this to be able to write correctly synchronized programs
: talk: in a weakly ordered memory models to not to have a full overhead of
: talk: reduction a multi threaded program to the sequentially executed single one.

* How it all applies to the programs

    int a = 0
    -----------------
    a = 1   |   a = 2
    r1 = a  |

    ----------------------------------------------------

    𝑤(𝑎, 1) −hb−> 𝑟(𝑎) : 1 ... 𝑤(𝑎, 2)
    𝑤(𝑎, 1) -hb−> 𝑟(𝑎) : 2 ... 𝑤(𝑎, 2)

                    [w(a, 1), r(a): 1, w(a, 2)]
                    [w(a, 1), w(a, 2), r(a): 2]

                                            𝑟1 ∈ {1, 2}

    Original -MM-> {Executions} -yield-> {Outcomes}
    Program                                 ^
      |                                     | (subset of)
      \-impl-> {Impls subset} -yields-> {Results}

                mov 1 → (a)                 𝑟1 ∈ {1}
                mov 1 → (r1)

.html mm/source_alex.html

: talk: An example of where various parts of the memory model stands.
: talk: As can be seen there are 2 flows in the plot. The upper one
: talk: reflects possible options and the bottom one reflects the
: talk: implementation subset.

: talk: It is seen that program has a set of possible executions
: talk: on specified abstract machine accordingly to the memory
: talk: model of that machine. Those executions yields a set
: talk: of related outcomes.

: talk: Then the implementation is happened to be a specific case
: talk: of all possible variants and produced only part of available
: talk: executions. Thus the result set is smaller.

: talk: Now let's take a look at what we have in Go with memory model.

* Go

Is something at [[https://golang.org/ref/mem][golang/mem]] a spec?

> "serialize accesses"
> "happens-before"
> "dependency edges"

: talk: Having read what we have at this url let's answer a question
: talk: is the Go memory model a specification?

* Go

Is something at [[https://golang.org/ref/mem][golang/mem]] a spec?

> "serialize accesses"
> "happens-before"
> "dependency edges"

- No

No correct synchronization notion, nothing about data races...

: talk: It seems like no. There is a notice that concurrent accesses
: talk: must be serialized, happens-before notion to define whether
: talk: a program has data race and few rules on the implementation...

: talk: But what the point then?

* What's the point?

Two purposes:

- Make guarantees for programmers.
- Allow compilers/hardware to make certain changes to programs.

Ideally, perfectly balanced. In practice, more conservative: might do neither.

Explicit concern: Leave room for future refinement, refraining from:

- making debatable guarantees to programmers
- allowing compilers/hardware to make debatable changes to programs

.html mm/source_dave.html

: talk: The point is to have something to define developers expectations,
: talk: something to define data races in the program, provide guarantees,
: talk: allow compiler and hardware to make changes to the programs but still
: talk: to keep room for future refinement.

* How to serialize access?

Order memory accesses:

- Explicitly given: intra-thread semantics preserved, HB, DE = (use sync primitives, channels, and goroutines start)
- Implicitly implied: PO, SO, SO-PO, commutativity, coherence, causality, consensus.

: talk: Go memory model defines happen-before order of the events to
: talk: define which reads sees what writes. There is also intra-thread
: talk: semantics and dependency edges. All other properties, orderings
: talk: and consistency rules are bore in mind.

: talk: HB consistency - r is guaranteed to observe w if both of the following hold:
: talk: w happens before r, any other write to the shared variable v either
: talk: happens before w or after r (no interleaving writes).

: talk: And that's it.

* What Synchronization-With order do we have?

    package q

    func init() {}  --hb-\
                         |
    ---                  |
                         |
    package p            |
                         |
    import "q"           |
                         |
    func init() {}  <----/


- If p imports q, q’s init happens before p’s.
- Package main’s init happens before main.main

Valid executions:

    [q.init..., p.init...]

: talk: Package import action induces a synchronized with relation between
: talk: participating packages between their `init` function, so that in
: talk: this example package p imports package q what induces hb relation
: talk: q.init < p.init (q.init -hb> p.init).

* What Synchronization-With order do we have?

    var a string

    func f() {
        print(a)    <--hb---\
    }                       |
                            |
    func hello() {          |
        a = "hello, world"  |
        go f()      --------/
    }

- The go statement happens before the created goroutine’s execution

Valid executions:

    [store(a, "hello, world"), read(a): "hello, world"]

: talk: `go` statement induces synchronized with relation between the `go`
: talk: action and the first action of the goroutine such that goroutine
: talk: scheduling happens before any first actions of the goroutine.
: talk: The thing is the goroutine termination does not set any SW edge.

* What Synchronization-With order do we have?

    var c = make(chan int, 10)
    var a string

    func f() {
        a = "hello, world"<-\
        c <- 0  ----------- | -\
    }                      hb  |
                            |  |
    func main() {           |  |
        go f()  ------------/  |
        <-c     <---hb---------/
        print(a)
    }

- A send (or close) on a channel happens before the receive

Valid executions:

    [store(a, "..."), send(c, 0), recv(c): 0, read(a): "..."]

: talk: A channel write induces synchronized with relation between channel
: talk: write and channel read such that channel send (or close) happens before
: talk: the receive.

* What Synchronization-With order do we have?

    var l sync.Mutex
    var a string

    func f() {
        a = "hello, world" -\
        l.Unlock()  <------ | -\
    }                       |  |
                            |  |
    func main() {           |  |
        l.Lock()            |  |
        go f()  <----hb-----/  |
        l.Lock()    -----hb----/
        print(a)
    }

- Unlock happens before subsequent Lock

Valid executions:

    [lock(l), store(a, "..."), unlock(l), lock(l), read(a): "..."]

: talk: A mutex lock has a synchronization with relation to the unclock
: talk: such that unlocks happens before subsequent locks.

* Hmmm...

: talk: And that's it. What is strange here?

: talk: The SW edges are defined not with an actions of the abstract machine
: talk: but with a specific runtime implementation synchronization primitives.

> Unlock happens before subsequent Lock

* How the Mutex works as a memory barrier?

SW relation is given in terms of runtime library.

: talk: There is also nothing about Causality guarantees while having HB.

: talk: Let's take a look how the mutexes are implemented then.

* How the Mutex is implemented?

    // Fast path: grab unlocked mutex.
    if atomic.CompareAndSwapInt32(&m.state, 0, mutexLocked) {...}

    // Long path:
    runtime_canSpin() -> runtime_doSpin()
    + starvation detection: runtime_nanotime()-waitStartTime > starvationThresholdNs
    + lock fairness with LIFO/Q semaphore runtime_SemacquireMutex(&m.sema, lifo: waitStartTime != 0)
    + race detection

sync/mutex is implemented with sync/atomic only.

: talk: Mutexes are implemented with a sync/atomic operations only.
: talk: But where is atomics spec in the memory model? If there are
: talk: no any spec on the well-formed executions then the primitives
: talk: them selves must work as a memory barriers and the compiler must
: talk: not change program behaviors (because he does not know).

* How the Mutex works as a memory barrier?

the sync/atomics works as a memory barriers.

: talk: That means that atomics implementation must work as a fence.
: talk: They must prevent reordering for normal read/writes across the
: talk: fence borders.

* What about sync/atomic in mm?

?

: talk: What about sync/atomic in mm?

* What about sync/atomic?

Package sync/atomic is conspicuously missing.

[[https://github.com/golang/go/issues/5045#issuecomment-252730563][Proposal]]: match atomics to Java volatile and C++ memory_order_seq_cst.

- An atomic write happens before an atomic read that observes the write.
- The outcome of atomic operations must be consistent with a total order over all atomic operations in the program.

.link https://github.com/golang/go/issues/5045 Issue: define how sync/atomic interacts with memory model #5045

: talk: Package sync/atomic is conspicuously missing.

: talk: One of the proposals is to match java volatile and c++ memory order seq cst.
: talk: But the issue is still open. The @rsc states that the described behavior
: talk: of the higher order primitives is enough. Yet let's take a look at the
: talk: atomics implementation.

* (A) Roots of the Go atomics semantics requirements

    // goroutine 1
    for atomic.Load(&stop) == 0 {...}

    // goroutine 2
    atomic.Store(&stop, 1)

> "Stop flag", no associated data, requires no memory ordering (only atomicity and visibility):

.link https://github.com/golang/go/issues/5045#issuecomment-169952557 @dvyukov comment from #5045, > there are three practically interesting synchronization patterns wrt memory ordering (in increasing order of required synchronization)

: talk: @dvyukov thoroughly described reasons behind ordering requirements behind the atomics
: talk: implementation with these great 3 cases third of which is the strongest and can't be
: talk: taken away.

* (B) Roots of the Go atomics semantics requirements

    // goroutine 1 (producer)
    data = 42
    atomic.Store(&ready, 1)

    // goroutine 2 (consumer)
    if atomic.Load(&ready) != 0 {
        assert(data == 42)
    }

> Producer-consumer, there is associated data so it requires acquire/release pair, but synchronization is weak and causal (no barriers on x86) (this is comment 15, program 1):

.link https://github.com/golang/go/issues/5045#issuecomment-169952557 @dvyukov comment from #5045, > there are three practically interesting synchronization patterns wrt memory ordering (in increasing order of required synchronization)

* (C) Roots of the Go atomics semantics requirements

    // goroutine 1
    atomic.Store(&X, 1)
    r1 = atomic.Load(&Y)

    // goroutine 2
    atomic.Store(&Y, 1)
    r2 = atomic.Load(&X)

    // afterwards
    if r1 == 0 && r2 == 0 {
      panic("broken")
    }

> Dekker/Peterson mutual exclusion, no associated data per se (but can be combined with (B)), strong synchronization (loads and stores allow to achieve global consensus, requires barriers on x86).

> we can't discard case (C). That's a fundamental pattern. *We used it* in sigqueue (with sig.mask and sig.kick being A and B); in sync.Waitgroup (with wg.counter and wg.waiters being A and B); in runtime semaphores (with addr and root.nwait being A and B); in several places in netpoll and runtime scheduler. Things will break if we take out that barrier.

.link https://github.com/golang/go/issues/5045#issuecomment-169952557 @dvyukov comment from #5045, > there are three practically interesting synchronization patterns wrt memory ordering (in increasing order of required synchronization)

: talk: This Dekker/Peterson pattern serves as basic building block for a lot of algorithms and primitives in runtime library.
: talk: That's why we have following implementation.

* How atomics works as a memory barrier?

on x86/amd64

    TEXT ·CompareAndSwapUint32(SB),NOSPLIT,$0-17
        MOVQ	addr+0(FP), BP
        MOVL	old+8(FP), AX
        MOVL	new+12(FP), CX
        LOCK                    <---------
        CMPXCHGL	CX, 0(BP)
        SETEQ	swapped+16(FP)
        RET

they use LOCK-prefixed instructions like CMPXCHG, XCHGQ, XADDL

: talk: They use LOCK-prefixed instructions like CMPXCHG, XCHGQ, XADDL.
: talk: XCHGQ on x86 is implicitly prefixed with LOCK. And having a LOCK
: talk: flag on an instructions means...

* How atomics works as a memory barrier?

on x86/amd64

Section 8.2.5:

    The I/O instructions, locking instructions, the LOCK prefix, and
    serializing instructions force stronger ordering on the processor.

Also from 8.2.5:

    "Like the I/O and locking instructions, the processor waits until
    all previous instructions have been completed and all buffered writes
    have been drained to memory before executing the serializing instruction."

.link https://stackoverflow.com/questions/50280857/do-locked-instructions-provide-a-barrier-between-weakly-ordered-accesses
.link https://peeterjoot.wordpress.com/2009/12/04/intel-memory-ordering-fence-instructions-and-atomic-operations/

: talk: The I/O instructions, locking instructions, the LOCK prefix, and
: talk: serializing instructions force stronger ordering on the processor.
: talk: Simply put LOCKed instruction serializes IO ops (waits all prev.
: talk: instructions to complete and all buffered writes to be drained).

* How atomics works as a memory barrier?

on arm64

    TEXT ·CompareAndSwapUint32(SB),NOSPLIT,$0-17
        MOVD	addr+0(FP), R0
        MOVW	old+8(FP), R1
        MOVW	new+12(FP), R2
    again:
        LDAXRW	(R0), R3     <------------ implicit
        CMPW	R1, R3
        BNE	ok
        STLXRW	R2, (R0), R3 <------------ rel/acq mem barrier
        CBNZ	R3, again
    ok:
        CSET	EQ, R0
        MOVB	R0, swapped+16(FP)
        RET

: talk: and the same on the arm64 with exclusive release acquire implicit mem barrier

* How atomics works as a memory barrier?

on arm

    TEXT ·armCompareAndSwapUint32(SB),NOSPLIT,$0-13
        ...
    casloop:
        // LDREX and STREX were introduced in ARMv6.
        LDREX	(R1), R0
        CMP	R0, R2
        BNE	casfail
        DMB_ISHST_7
        STREX	R3, (R1), R0
        CMP	$0, R0
        BNE	casloop
        MOVW	$1, R0
        DMB_ISH_7  <--------- explicit memory barrier
        MOVBU	R0, swapped+12(FP)
        RET
    casfail:
        MOVW	$0, R0
        MOVBU	R0, swapped+12(FP)
        RET

: talk: and the same on arm with explicit memory barrier

* How atomics works as a memory barrier?

on arm

- Load-Acquire (LDAR): All loads and stores that are after an LDAR in program order, and that match the shareability domain of the target address, must be observed after the LDAR.
- Store-Release (STLR): All loads and stores preceding an STLR that match the shareability domain of the target address must be observed before the STLR.
- There are also exclusive versions of the above, LDAXR and STLXR, available.

.link https://developer.arm.com/products/architecture/a-profile/docs/100941/latest/barriers
.link http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/CEGDBEJE.html
.link https://stackoverflow.com/questions/21535058/arm64-ldxr-stxr-vs-ldaxr-stlxr
.link http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/ch06s03s11.html ARM Cortext-A > Multi-processing systems > Synchronization

* Data races

    var a string

    func hello() {
        go func() { a = "hello" }()
        print(a)
    }

> The exit of a goroutine is not guaranteed to happen before any event in the program.

Valid executions:

    [read(a)]
    [read(a), store(a)]
    [store(a), read(a)]

: talk: An example of the data race induced by unsynchronized read in the goroutine.
: talk: The goroutine exit does not imply SW relation. It could even to not to begin
: talk: executing before the program termination.

* Data races

.code mm/print_ab.go /func f/,

> it can happen that g prints 2 and then 0.

One of the valid executions:

    [store(b, 2), read(b): 2, read(a): 0, store(a, 1)]

: talk: An example of the data race induced by unsynchronized independent writes and
: talk: independent reads.

* Data races

.code mm/wait_idiom.go /func setup/,

> there is no guarantee that, in doprint, observing the write to done implies observing the write to a. This version can (incorrectly) print an empty string instead of "hello, world", and can finish not.

One of the valid executions:

    [store(done, true), read(done), read(a): "", store(a, "...")]

: talk: The same as previous example but it's even worse. There is no guarantee
: talk: that the write to done will ever be observed by main, since there are no
: talk: synchronization events between the two threads.
: talk: The loop in main is not guaranteed to finish.

* Data races

.code mm/twoprints.go /func setup/,

> As before, there is no guarantee that, in main, observing the write to done implies observing the write to a, so this program could print an empty string too.

    [t1: read(done): false, t1: store(done, true), t2: read(done): true, t2: read(a): "", t1: store(a, "..."), t1: read(a): ...]

: talk: An invalid attempt of double-checked locking. Observing the write to done
: talk: does not guarantee observing the write to a, because writes and reads to the done and a
: talk: are not synchronized at all.

* Data races

.code mm/unsafe_construct.go /type T/,

> Even if main observes g != nil and exits its loop, there is no guarantee that it will observe the initialized value for g.msg.

* Main advice from Go team

> use explicit synchronization

: talk: Don't be too smart. Don't try to hack rules.

* Detection of data races

- [[http://www.es.mdh.se/pdf_publications/4327.pdf][static]] detectors: flow-based, inferring types constraints graph solvers, model checking
- dynamic detectors: [[http://www.cis.upenn.edu/~stevez/papers/MPZD17.pdf][vector clocks tracking]] (requires instrumentation), [[http://www.cs.williams.edu/~freund/papers/10-cacm.pdf][FastTrack]]
- mixed stuff

* TSAN

Go race detector is implemented via [[https://github.com/google/sanitizers/wiki/ThreadSanitizerAlgorithm][TSAN]] which
is classic vector clock detector.

- it uses CGO to bind to TSAN
- TSAN is an external LLVM library at http://llvm.org/git/compiler-rt.git (68e1532492f9b3fce0e9024f3c31411105965b11)
- Go provides callbacks to be called from C code (instruction to source mapping)
- There are optimizations to call TSAN lib directly (not via std cgo path)
- Main bindings are in `runtime` package, `runtime/race`
- Go ships with prebuild TSAN binaries
- requires ASLR+PIE, mmaps shadow memory, amd64

* Race detector implementation

- Go: [[https://github.com/golang/go/blob/master/src/runtime/race.go][race.go]], [[https://github.com/golang/go/blob/master/src/runtime/race_amd64.s][race_amd64.s]]
- TSAN: [[https://github.com/llvm-mirror/compiler-rt/blob/master/lib/tsan/go/tsan_go.cc][tsan_go.cc]], everything else in RTL, DD
- Algorithm: 1) instrument all actions with rumtime.raceread, runtime.racewrite... 2) if wrt action to the same location in different threads => compare accessor vector clocks with the scalar clock of the shadow history (to detect invalid ordering (missing happens-before edges)) and update randomly selected shadow state
- uses ShadowState to track limited history of memory accesses
- uses MetaMap to track synchronization events
- uses ThreadState to track thread clocks (kept in g)
- only accessor clock kept in ShadowState instance

* Race detector evaluation

- No false positives
- Depends on execution traces (can miss traces)
- 5x-15x slowdown by CPU
- 5x-10x memory usage

* Links

- "The Go Memory Model", Version of May 31, 2014, https://golang.org/ref/mem
- "Go’s Memory Model", Russ Cox, rsc@google.com, MIT 6.824 / February 25, 2016
- "Using JDK 9 Memory Order Modes", Doug Lea, March 25 2018
- "Java Memory Model Unlearning Experience", Aleksey Shipilёv, shade@redhat.com, @shipilev, 2018
- "Close Encounters of The Java Memory Model Kind", Aleksey Shipilёv, 2016
- "Java Memory Model Pragmatics (transcript)", Aleksey Shipilёv, 2014
- "C++ Memory Model", https://en.cppreference.com/w/cpp/language/memory_model

* Links

- [[https://github.com/golang/go/issues/5045][doc: define how sync/atomic interacts with memory model #5045]]
- [[https://stackoverflow.com/questions/50280857/do-locked-instructions-provide-a-barrier-between-weakly-ordered-accesses]]
- [[https://peeterjoot.wordpress.com/2009/12/04/intel-memory-ordering-fence-instructions-and-atomic-operations/]]
- [[https://developer.arm.com/products/architecture/a-profile/docs/100941/latest/barriers]]
- [[http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/CEGDBEJE.html]]
- [[https://stackoverflow.com/questions/21535058/arm64-ldxr-stxr-vs-ldaxr-stlxr]]
- [[http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/ch06s03s11.html][ARM Cortext-A > Multi-processing systems > Synchronization]]

* Links

- https://github.com/google/sanitizers/wiki/ThreadSanitizerAlgorithm
- https://github.com/google/sanitizers/wiki/ThreadSanitizerPopularDataRaces
- https://github.com/golang/go/blob/master/src/runtime/race.go
- http://llvm.org/git/compiler-rt.git

* Quotes

> the whole formalism of happens-before exists in order to define precisely the terms under which sequentially inconsistent executions can maintain an illusion of sequential consistency.

.link https://stackoverflow.com/questions/12018369/whats-sequentially-consistent-executions-are-free-of-data-races/12018485#12018485 by Marko Topolnik from Aug 18 '12 at 12:18
