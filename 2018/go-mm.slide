Go Memory Model
Jule, 2018
Tags: go, memory, model

Ivan Prisyazhnyy
Software Engineer
@JohnKoepi

: talk: Good day!
: talk: How is everybody doing today?
: talk: ...
: talk: Cool!
: talk: ...
: talk: My name is Ivan and I am here today to talk to you about
: talk: Go memory model, and more specifically the importance of
: talk: the memory model, why do we need one, how to understand it
: talk: and how to apply it.
: talk:
: talk: You will also know what are data races, what memory models
: talk: exists, how it is implemented, what are incorrectly synchronized
: talk: programs and how to detect data races.
: talk:
: talk: The talk consists of the three parts:
: talk: 1) First one introduces a memory models and answers the
: talk:    question WHY. WHY SHOULD YOU CARE ABOUT A MEMORY MODEL?
: talk:    Why Go needs memory model at all. What memory models
: talk:    exists. What are hardware memory models. What compilers do.
: talk: 2) Second part introduces memory models THEORY.
: talk:    You will know how memory models are defined, what they
: talk:    consists of, how the model reflects the program and its
: talk:    states. It's about a happens-before, consistency and DRF.
: talk: 3) And in the third one you will get a glimpse of the details
: talk:    of the MEMORY MODEL in GO. What it is based on? What it
: talk:    describes and how. Is it a full spec or not? How atomics
: talk:    works.
: talk: 4) In top secret part four there will be examples of data
: talk:    races and bad synchronization examples. And then few words
: talk:    on the RACE DETECTION and `tsan` implementation.

* Opinions are my own

Anything on this or any subsequent slides may be a lie. Do
not base your decisions on this talk.

.link https://golang.org/ref/mem#tmp_1 don't be clever advice

: Do you like statements when people do not hold any responsibility?

: talk: Anything on this or any subsequent slides may be a lie. Do
: talk: not base your decisions on this talk.

* A memory model?

A thing from

.link https://golang.org/ref/mem

version of May 31, 2014

: talk: What is a memory model? Isn't it just a thing documented at this
: talk: link? version from May 31, 2014.

* How to answer a question:

What is a correct program?

- Execution traces - histories of actions,
- [[https://en.oxforddictionaries.com/definition/behaviour][Behaviors]] - the ways in which program acts,
- Outcomes - results.

: A program consists of actions which constitutes executions.
: The executions outcomes with results. And the executions
: constitutes program behaviors.

: talk: There is no questions when we are writing single threaded programs.
: talk: But let's assume we are writing concurrent programs.

: talk: A memory model is a thing that helps to answer a question: what
: talk: is a correct program? - what executions of the program are valid.
: talk: To be more precise a memory model allows a developer to write
: talk: correctly synchronized programs.

: talk: In the context of memory models, programs consists of actions, executed
: talk: actions constitutes program executions (or execution traces),
: talk: and the behaviors are the effects of executed actions.

: talk: But, why should we care?

* Why should we care?

.code mm/print_ab.go /START OMIT/,/END OMIT/

- Is that correct program?
- What are possible outcomes?

: talk: Let's take a look at this code example. Try to guess, is that program
: talk: correct? What are possible results of this program?

: talk: How to answer such question any way?

* Why should we care?

.code mm/print_ab.go /START OMIT/,/END OMIT/

- Is that correct program?
- What are possible outcomes?
- Is (2, 0) a possible outcome?

: talk: While answering previous questions, you also have to answer i.e. is
: talk: (2, 0) outcome is possible. Is it possible? How to understand it?

* Why should we care?

.code mm/wait_idiom.go /START OMIT/,/END OMIT/

- Is that correct program?
- What are possible outcomes?

: talk: Ok, what about this one? Is that program correct? What are possible
: talk: results of this program?

: talk: So, remember your answers. Now, let's move on to the question:
: talk: What is a correct program anyway?

* Race Condition

"A race condition is the behavior of the system where the output
is dependent on the sequence or timing of other uncontrollable events.
It becomes a bug when events do not happen in the order the programmer intended."

.link https://en.wikipedia.org/wiki/Race_condition Wikipedia (def.)

: some kind of external timing or ordering non-determinism

: talk: There is a notion of race condition, which is defined as follows.

* Data Race

"a race condition caused by potentially concurrent operations on a shared memory location, of which at least one is a write."

.link https://en.wikipedia.org/wiki/Race_condition#Software Wikipedia/Software/C++ x11

: talk: And there is a notion of data race.

* Race Conditions != Data Races,

It's not just race conditions are about _events_, and the data races are about the events of memory  _accesses_.

.link https://blog.regehr.org/archives/490 Race Condition vs. Data Race

Data Races are *bad*.

.link https://stackoverflow.com/questions/11276259/are-data-races-and-race-condition-actually-the-same-thing-in-context-of-conc

The correctly synchronized program is data race free.

: talk: The thing is the data races are bad, while race conditions are not
: talk: always bad. They are confused sometimes, so don't be.
: talk: The correctly synchronized program is data race free.

* Example of data race and race condition

    var count = 0                       var count = 0
                                        var m = sync.Mutex{}
    func incrementCount() {             func set(x int) {
        if count == 0 {                     m.Lock()
            count ++                        count = x
        }                         vs        m.Unlock()
    }                                   }

    func main() {                       func main() {
        go incrementCount()                 go set(1)
        go incrementCount()                 go set(2)
    }                                   }

- What are possible outcomes?
- Where is a DR, where is a RC?

: talk: Take a look at following code examples. How do you think, where
: talk: it is a DR and where it is a RC? Are they equally harmful?

: talk: Both are capable of producing 2 or 1. But are they equally harmful?

* Data Race Free (DRF)

- "A program must be correctly synchronized to avoid the kinds of counterintuitive behaviors that can be observed when code is reordered",
- "A program is correctly synchronized if and only if all sequentially consistent executions are free of data races",
- "If a program is correctly synchronized, then all executions of the program will appear to be sequentially consistent".

: talk: Knowing data races are bad we can conclude a notion of a correctly
: talk: synchronized program. Simply put, its a program that is free of data races.
: talk: Then all program executions appear to be sequentially consistent and
: talk: vice versa.

.link https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.4 JMM

* Sequential Consistency

Sequential Consistency (SC): (def.)
¬´...the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program¬ª

> If a program has no data races, then all executions of the program will appear to be sequentially consistent.

: talk: Where seq. cons. is a situation in which an execution is consistent with PO.

* How can we understand that the program is correct?

Just make it DRF (data race free).

- In order of doing so, you must made it correct under a memory model.
- Memory models specifies which executions of the program are correct.

: talk: So now we know what is a correctly synchronized program.

* How can we understand that the program is correct?

> Use memory model, Luke.

- which programs are correct
- what those programs can do
- therefore what programmers can expect
- therefore what compiler writers must ensure / can do

: talk: Memory Model sets our expectations.

* History intro

: talk: The question: Why we can't live without another level
: talk: of abstraction?

: talk: Let's see how different levels of execution affects
: talk: program behaviors.

* A Fairy Tale

In a past good days everything was simple.

Once upon a time‚Ä¶

How do you make a program faster?

    Buy newer hardware.

Is a hardware or compiler optimization valid?

    Yes, if valid programs don‚Äôt change behavior.
    (And most programs are valid.)

.html mm/source_dave.html

: talk: In a past good days everything was simple.
: talk: Single core chips, no concurrent architectures,
: talk: simple compilers, simple conveyors, simple caches,
: talk: new hardware is valid if it does not change behavior.

* An Evil Twist

Hardware engineer‚Äôs magic spells stopped working.

    New magic spell: stamp out more and more cores.

Compiler and operating system engineers give us multithreaded programming.

Now hardware, compiler optimizations change program behaviors.

- multiple issue of instructions
- out-of-order execution
- speculative execution
- optimizations on various levels
- multiple core, caches

.html mm/source_dave.html

: talk: At one moment, everything changed. Hardware guys started to
: talk: build multi cores systems. Compilers and OSes became complicated
: talk: software performing complex optimizations. Multithreaded
: talk: programming appeared. And since then the new paradigm was
: talk: the hardware and compilers change program's behavior.

* How can we get along?

‚ÄúValid optimizations do not change the behavior of valid programs.‚Äù

: talk: So, how can we get along all this new fancy stuff?

back to our code example:

.code mm/wait_idiom.go /START OMIT/,/END OMIT/

Can it ever print 0?

.html mm/source_dave.html

: talk: Back to our code example, can this program ever print 0?

* Can it ever print 0?

‚ÄúIt depends.‚Äù

: talk: And the answer is - it depends.

.code mm/wait_idiom.go /START OMIT/,/END OMIT/

In x86 assembly, *no*.
In ARM/POWER assembly, *yes*.
In most C compilers (even on x86), *yes* (or maybe it won‚Äôt even finish).

: talk: Depending on the hardware architecture and compiler the answer
: talk: varies.

.html mm/source_dave.html

* ‚ÄúIt depends‚Äù is not a happy ending

    Hardware: x86, arm, ...
    ---------------------------------\
      Abstract Machine: ir, jvm      |
      Compiler: gcc, llvm, ic, jvm   | <- Memory Model of our abstract machine
      Language: asm, c, java, go     |    is somewhere there...
    ---------------------------------/
    Program

.html mm/source_dave.html

: talk: And the answer "it depends" is not a happy ending.

: talk: Before getting to the language memory model we need to
: talk: look at the hardware level memory models and understand
: talk: what guarantees it provides.

* Hardware Memory Models

: Memory ordering describes the order of accesses to computer memory by a CPU. The term can refer either to the memory ordering generated by the compiler during compile time, or to the memory ordering generated by a CPU during runtime.

: Runtime memory ordering

: In symmetric multiprocessing (SMP) microprocessor systems

: talk: Let's imagine we are writing with an assembly language.
: talk: How badly can the hardware mess with us?

: talk: Let's start with a strong consistency guarantees - sequential consistency.

* 1. Sequential Consistency

> all reads and all writes are in-order

    ‚ÄúThe customary approach to designing and proving the correctness of
    multiprocess algorithms for such a computer assumes that the following condition
    is satisfied: the result of any execution is the same as if the operations of all the
    processors were executed in some sequential order, and the operations of each
    individual processor appear in this sequence in the order specified by its program.
    A multiprocessor satisfying this condition will be called sequentially consistent.‚Äù

‚Äî Lamport, ‚ÄúHow to Make a Multiprocessor Computer That Correctly Executes
Multiprocess Programs‚Äù (1979)

: talk: By def. a seq. cons. is an approach in designing computer programs such that
: talk: all of its executions appears equivalent to some sequential execution consistent
: talk: with the program order of instructions. or in other words all RWS are in-order.

* Litmus Test: Message Passing

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  r1 = y
      y = 1   |  r2 = x

Can this program see r1 = 1, r2 = 0?

.html mm/source_dave.html

: talk: Can this program see r1 = 1, r2 = 0?

* Message Passing w/ Sequential Consistency

.html mm/sce_msg_pass.html

Can this program see r1 = 1, r2 = 0?

On sequentially consistent hardware: *no*.

.html mm/source_dave.html

: talk: Applying this definition to our example the answer is NO for seq. con. architecture.
: talk: On seq. con. arch. it can't reorder events in the same thread. So the writes from t1
: talk: can't be observed in different order by t2, because execution in which it happens to
: talk: be [... y=1 ... x=1 ...] is not consistent to the PO.

* Sequentially Consistent Hardware

.image mm/hw_sc.png

Maranget et al.,
.link https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf ‚ÄúA Tutorial Introduction to the ARM and POWER Relaxed Memory Models‚Äù

: talk: This is how this architecture looks like. It was that simple in a past good days.

* 2. Relaxed ordering: x86 Hardware (Total Store Order)

.image mm/hw_sto.png _ 650

Maranget et al.,
.link https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf ‚ÄúA Tutorial Introduction to the ARM and POWER Relaxed Memory Models‚Äù

: talk: Now take a look at how x86 architecture looks like. Do you see a difference? It has a
: talk: store buffers, and global write lock which provides TSO architecture type.

* Short note on TSO architecture

Relaxed consistency (some types of reordering are allowed)

- Loads can be reordered after loads (for better working of cache coherency, better scaling): no
- Loads can be reordered after stores: no
- Stores can be reordered after stores: no
- Stores can be reordered after loads: yes

.link https://en.wikipedia.org/wiki/Memory_ordering

: talk: The difference of TSO architecture which is a more relaxed memory ordering than seq. con.
: talk: is that stores can be reordered after loads.

* Litmus Test: Message Passing

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  r1 = y
      y = 1   |  r2 = x

Can this program see r1 = 1, r2 = 0?
On sequentially consistent hw: no.
On x86 (or other TSO): *no*.

    Thread 1‚Äôs writes are observed by other threads in original order.

.html mm/source_dave.html

: talk: Can this program see r1 = 1, r2 = 0?
: talk: No, thread 1‚Äôs writes are observed by other threads in original order,
: talk: because writes are not reordered.

* Litmus Test: Store Buffering

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  y = 1
      r1 = y  |  r2 = x

Can this program see r1 = 0, r2 = 0?
On sequentially consistent hw: *no*.
On x86 (or other TSO): *yes*!

    Thread 1‚Äôs local writes are not immediately visible in Thread 2 (and vice versa).

: talk: Can this program see r1 = 0, r2 = 0?
: talk: On seq. con. hw. no, and on x86 yes, thread 1‚Äôs local writes are not
: talk: immediately visible in thread 2 (and vice versa).

.html mm/source_dave.html

* Memory Fences

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  y = 1
      *fence* |  *fence*
      r1 = 1  |  r2 = x

Can this program see r1 = 1, r2 = 0? No.

Memory fence ensures Thread 1‚Äôs write is globally visible before Thread 2‚Äôs read,
and vice versa.

.html mm/source_dave.html

: talk: To help to solve code synchronization problems there are memory fences.
: talk: Can this program see r1 = 1, r2 = 0? No. Memory fence ensures thread 1‚Äôs
: talk: write is globally visible before Thread 2‚Äôs read, and vice versa.

* Litmus Test: Independent Reads of Indep. Writes

    Thread 1 | T2    | T3     | T4
    ---------------------------------
    x = 1    | y = 1 | r1 = x | r3 = y
                       r2 = y | r4 = x

Can this program see r1 = 1, r2 = 0, r3 = 1, r4 = 0?

Can Thread 3 see x change before y but Thread 4 see the opposite?

On sequentially consistent hw: no.
On x86 (or other TSO): no.

There is a total order over all stores to main memory.

.html mm/source_dave.html

: talk: Another litmus test on total order of all store in main memory.
: talk: Can Thread 3 see x change before y but Thread 4 see the opposite?
: talk: The answer is no on totally ordered writes architectures.

* 2. Relaxed ordering: ARM/POWER Hardware

.image mm/hw_arm.png _ 450

Maranget et al.,
.link https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf ‚ÄúA Tutorial Introduction to the ARM and POWER Relaxed Memory Models‚Äù

: talk: Now let's take a look at ARM architecture. There are independent
: talk: cores with independent memories with gossip broadcast coherence
: talk: protocols.

: talk: ARM is not TSO. It can reorder loads and stores.

* Litmus Test: Message Passing

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  r1 = y
      y = 1   |  r2 = x

Can this program see r1 = 1, r2 = 0?
On sequentially consistent hw: no.
On x86 (or other TSO): no.
On ARM/POWER: *yes*!

    Thread 1‚Äôs writes may not be observed by other threads in original order.

.html mm/source_dave.html

: talk: Back to our litmus test 1. Can this program see r1 = 1, r2 = 0?
: talk: Yes for the ARM architecture. Thread 1‚Äôs writes may not be observed
: talk: by other threads in original order.

* Litmus Test: Store Buffering

         var x, y int
    ----------------------
        T1    |      T2
    ----------------------
      x = 1   |  y = 1
      r1 = y  |  r2 = x

Can this program see r1 = 0, r2 = 0?
On sequentially consistent hw: no.
On x86 (or other TSO): yes!
On ARM/POWER: *yes*!

    Thread 1‚Äôs local writes are not immediately visible in Thread 2 (and vice versa).

.html mm/source_dave.html

: talk: Litmus test 2. Can this program see r1 = 0, r2 = 0? It's yes for ARM.
: talk: Thread 1‚Äôs local writes are not immediately visible in Thread 2 (and vice versa).

* Litmus Test: Independent Reads of Indep. Writes

    Thread 1 | T2    | T3     | T4
    ---------------------------------
    x = 1    | y = 1 | r1 = x | r3 = y
                       r2 = y | r4 = x

Can this program see r1 = 1, r2 = 0, r3 = 1, r4 = 0?

(Can Thread 3 see x change before y but Thread 4 see the opposite?)

On sequentially consistent hw: no.
On x86 (or other TSO): no.
On ARM/POWER: *yes*!

    Different threads may receive different writes in different orders.

.html mm/source_dave.html

: talk: Litmus test 3. Can Thread 3 see x change before y but Thread 4 see the opposite?
: talk: Yes on ARM. Different threads may receive different writes in different orders.

* Litmus Test: Coherence

    Thread 1 | T2    | T3     | T4
    ---------------------------------
    x = 1    | x = 2 | r1 = x | r3 = x
                       r2 = x | r4 = x

Can this program see r1 = 1, r2 = 2, r3 = 2, r4 = 1?

(Can Thread 3 see x=1 before x=2 but Thread 4 see the opposite?)

On sequentially consistent hw: no.
On x86 (or other TSO): no.
On ARM/POWER: *yes*!

    Different threads may receive different writes in different orders.

.html mm/source_dave.html

: talk: And thats an example of how coherence properties are different for these
; talk: architectures.

* 3. Weak Ordering

‚ÄúLet a synchronization model be a set of constraints on memory accesses that
specify how and when synchronization needs to be done.

Hardware is weakly ordered with respect to a synchronization model if and only if
it appears sequentially consistent to all software that obey the synchronization
model.‚Äù

.html mm/source_dave.html

.link http://pages.cs.wisc.edu/~markhill/papers/isca90_drf0.pdf Adve and Hill, ‚ÄúWeak Ordering - A New Definition‚Äù (1990)

: talk: and there is also third memory ordering kind a Weak ordering in which an
: talk: arbitrary reordering of stores and loads is possible. The order is preserved
: talk: only by explicit memory fences.

: The 13 possible strict weak orderings on a set of three elements {a, b, c}. The only partially ordered sets are coloured, while totally ordered ones are in black. Two orderings are shown as connected by an edge if they differ by a single dichotomy.
: In mathematics, especially order theory, a weak ordering is a mathematical formalization of the intuitive notion of a ranking of a set, some of whose members may be tied with each other. Weak orders are a generalization of totally ordered sets (rankings without ties) and are in turn generalized by partially ordered sets and preorders.

: Strict weak orderings
: A strict weak ordering is a binary relation < on a set S that is a strict partial order (a transitive relation that is irreflexive, or equivalently,[6] that is asymmetric) in which the relation "neither a < b nor b < a" is transitive.[1] Therefore, a strict weak ordering has the following properties:

: - For all x in S, it is not the case that x < x (irreflexivity).
: - For all x, y in S, if x < y then it is not the case that y < x (asymmetry).
: - For all x, y, z in S, if x < y and y < z then x < z (transitivity).
: - For all x, y, z in S, if x is incomparable with y (neither x < y nor y < x hold), and y is incomparable with z, then x is incomparable with z (transitivity of incomparability).
: This list of properties is somewhat redundant, in that asymmetry implies irreflexivity, and in that irreflexivity and transitivity together imply asymmetry.

: .link https://en.wikipedia.org/wiki/Weak_ordering

: Less formally, a less formal definition (Weak Consistency):

: Definition 1: In a multiprocessor system,storage accesses are weakly ordered if (1) accesses to global synchronizing variables are strongly ordered, (2) no access to a synchronizing variable is issued by a processor before all previous global data accesses have been globally performed, and if (3) no access to global data is issued by a processor before a previous access to a synchronizing variable has been globally performed.*

* Synchronization model: Data-Race-Free (DRF)

Synchronization operations are (to hardware) recognizably different from ordinary
operations.

A program is data-race-free if for all idealized SC executions, any two ordinary
memory accesses to the same location from different threads are either:

- both reads
- or separated by synchronization operations: one _happens-before_ the other

.html mm/source_dave.html

"We believe that DRF allows for faster hardware without significantly reducing software flexibility."

.link http://pages.cs.wisc.edu/~markhill/papers/isca90_drf0.pdf Adve and Hill, ‚ÄúWeak Ordering - A New Definition‚Äù (1990)

: talk: A program is data-race-free if for all idealized SC executions, any two ordinary
: talk: memory accesses to the same location from different threads are either:
: talk: both reads or separated by synchronization operations: _happens-before_ edge.

: talk: DRF sync model is a lightweight sync model to allow faster hardware without
: talk: significant overhead on sync and it does not put any constraints on the sync
: talk: primitives, but rather specifies whether there is enough of sync edges.

* Data-Race-Free

.image mm/drf_free.png _ 600

.link http://pages.cs.wisc.edu/~markhill/papers/isca90_drf0.pdf Adve and Hill, ‚ÄúWeak Ordering - A New Definition‚Äù (1990)

: talk: Example 1 of DRF program.

* Not Data-Race-Free

.image mm/drf_non_free.png _ 550

.link http://pages.cs.wisc.edu/~markhill/papers/isca90_drf0.pdf Adve and Hill, ‚ÄúWeak Ordering - A New Definition‚Äù (1990)

: talk: Example 2 of DRF program.

* Hardware weakly ordered by DRF

‚ÄúHardware is weakly ordered with respect to [DRF] if and only if it appears
sequentially consistent to all software that obey [DRF].‚Äù

.html mm/source_dave.html

.link http://pages.cs.wisc.edu/~markhill/papers/isca90_drf0.pdf Adve and Hill, ‚ÄúWeak Ordering - A New Definition‚Äù (1990)
.link https://herbsutter.com/2012/08/02/strong-and-weak-hardware-memory-models/

: talk: And here we have a definition of weakly ordered hardware with respect to
: talk: [DRF], and that means...

* Hardware weakly ordered by DRF when...

    - Intra-processor dependencies are preserved
    - All writes to same location have a global total order (coherence)
    - All sync operations to same location have a global total order
        - for S1 before S2, all of S1 must complete before any of S2 starts.
    - A new access is not generated by a processor until previous sync operations
    are committed.
    - Once a sync operation S by processor P is committed, no other sync
    operations on the same location can commit until:
        - all reads by P before S must be committed
        - all writes by P before S must be globally performed

by definition of Weak Consistency

.html mm/source_dave.html

: talk: that single threads dependencies are preserved, coherence is preserved,
: talk: sync ops to some location have total order, other ops respect sync ops.

* Hardware weakly ordered by DRF

Basically everything, given appropriate synchronization implementations

- Earlier hardware models
- VAX
- x86
- ARM/POWER

guarantee that DRF implies appearance of SC: DRF-SC.

.html mm/source_dave.html

: talk: Basically everything, given appropriate synchronization implementations
: talk: guarantee that DRF implies appearance of SC: DRF-SC.

* Compilers

Significant freedom to rewrite code.
Significant gaps in knowledge of execution.

    w = 1
    x = 2
    r1 = y
    r2 = z

Compiled code fails (answers *yes!* to) every litmus test we‚Äôve seen,
including coherence!

.html mm/source_dave.html

: talk: Now, let's move on to the compilers. Compilers are the next big thing which
: talk: can change programs behavior. They do that according to that abstract machine
: talk: they are working upon.

: talk: Compilers have significant freedom to rewrite code and they have
: talk: significant gaps in knowledge of execution.

* Compiler Optimizations

> Compiler always expects the programs are DRF

: talk: Compiler always expects the programs are DRF.

* Compiler Optimizations

Is this a valid optimization?

.html mm/compiler_opt_1.html

Compiler and language must be involved in multithreaded guarantees.

.link http://www.hpl.hp.com/techreports/2004/HPL-2004-209.pdf Boehm, ‚ÄúThreads Cannot be Implemented as a Library‚Äù (2004):

.html mm/source_dave.html

: talk: This is an example of questioning optimization bounds by the compiler.
: talk: Is this a valid optimization?

: talk: The main idea in the end is that compiler and language must be involved
: talk: in multithreaded guarantees.

* Weak Ordering?

‚ÄúHardware is weakly ordered with respect to a synchronization model if and only if
it appears sequentially consistent to all software that obey the synchronization
model.‚Äù

> Why not a programming language (implementation)?

.html mm/source_dave.html

: talk: And as a result the ideas on weak ordering with respect to the DRF-SC
: talk: on a hardware can be transposed to the programming language / compiler.

: talk: Ok, how do we define a language memory model?

* Theory

Memory model:

- defines which executions of a program are legal
- thus, allows to understand which optimizations are correct

"A program is correctly synchronized if and only if all sequentially consistent executions are free of data races."

"If a program is correctly synchronized, then all executions of the program will appear to be sequentially consistent (¬ß17.4.3)."

.link https://en.wikipedia.org/wiki/Memory_model_(programming)
.link https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.4

: talk: A memory model defines which executions of a program are legal. We need
: talk: a program to be correctly synchronized with respect to the memory model
: talk: to have expected behaviors (and results).

: talk: A program is correctly synchronized if and only if all sequentially
: talk: consistent executions are free of data races. And it works vice versa.

: talk: What are those program executions?

* Theory

- Program => Executions => Outcomes,
- `Program` consists of `Actions`,
- `Executions` consists of sequential traces of a program `Actions`,
- `Behaviours` may be incorrect and counterintuitive if something wrong.

Executions are the behaviors of the abstract machine, not
the behavior of final implementation. They define all possible
ways the program can possibly execute.

: Executions in JMM:

: An execution E is described by a tuple < P, A, po, so, W, V, sw, hb >, comprising:

: P - a program
: A - a set of actions
: po - program order, which for each thread t, is a total order over all actions performed by t in A
: so - synchronization order, which is a total order over all synchronization actions in A
: W - a write-seen function, which for each read r in A, gives W(r), the write action seen by r in E.
: V - a value-written function, which for each write w in A, gives V(w), the value written by w in E.
: sw - synchronizes-with, a partial order over synchronization actions
: hb - happens-before, a partial order over actions

: Note that the synchronizes-with and happens-before elements are uniquely determined by the other components of an execution and the rules for well-formed executions (¬ß17.4.7).
: An execution is happens-before consistent if its set of actions is happens-before consistent (¬ß17.4.5).

: Well-Formed Executions
: We only consider well-formed executions. An execution E = < P, A, po, so, W, V, sw, hb > is well formed if the following are true:
: 1. Each read sees a write to the same variable in the execution.
:    All reads and writes of volatile variables are volatile actions. For all reads r in A, we have W(r) in A and W(r).v = r.v. The variable r.v is volatile if and only if r is a volatile read, and the variable w.v is volatile if and only if w is a volatile write.
: 2. The happens-before order is a partial order.
:    The happens-before order is given by the transitive closure of synchronizes-with edges and program order. It must be a valid partial order: reflexive, transitive and antisymmetric.
: 3. The execution obeys intra-thread consistency.
:    For each thread t, the actions performed by t in A are the same as would be generated by that thread in program-order in isolation, with each write w writing the value V(w), given that each read r sees the value V(W(r)). Values seen by each read are determined by the memory model. The program order given must reflect the program order in which the actions would be performed according to the intra-thread semantics of P.
: 4. The execution is happens-before consistent (¬ß17.4.6).
: 5. The execution obeys synchronization-order consistency.
:    For all volatile reads r in A, it is not the case that either so(r, W(r)) or that there exists a write w in A such that w.v = r.v and so(W(r), w) and so(w, r).

: + Causality requirements $17.4.8

* Theory

Executions ‚âà Actions ‚à™ Orders ‚à™ Consistency Rules

Actions:

- Read (Load)
- Write (Store)
- Synchronization actions: volatile rw (seq_cst), acq/rel, lock/unlock, synthetic (first action of the goroutine), actions that start threads/goroutines or destroys thema
- External actions

: External Actions. An external action is an action that may be observable outside of an execution, and has a result based on an environment external to the execution.

- Divergent actions

: Thread divergence actions (¬ß17.4.9). A thread divergence action is only performed by a thread that is in an infinite loop in which no memory, synchronization, or external actions are performed. If a thread performs a thread divergence action, it will be followed by an infinite number of thread divergence actions.

: Thread divergence actions are introduced to model how a thread may cause all other threads to stall and fail to make progress.

* Theory

Executions ‚âà Actions ‚à™ Orders ‚à™ Consistency Rules

Orders:

- Program Order (PO)

: Among all the inter-thread actions performed by each thread t, the program order of t is a total order that reflects the order in which these actions would be performed according to the intra-thread semantics of t.

- Synchronization Order (SO)

: A synchronization order is a total order over all of the synchronization actions of an execution. For each thread t, the synchronization order of the synchronization actions (¬ß17.4.2) in t is consistent with the program order (¬ß17.4.3) of t.

- Happens-Before (HB)

: If we have two actions x and y, we write hb(x, y) to indicate that x happens-before y.
: If x and y are actions of the same thread and x comes before y in program order, then hb(x, y).
: There is a happens-before edge from the end of a constructor of an object to the start of a finalizer (¬ß12.6) for that object.
: If an action x synchronizes-with a following action y, then we also have hb(x, y).
: If hb(x, y) and hb(y, z), then hb(x, z).

"A set of synchronization edges, S, is sufficient if it is the minimal set such that the transitive closure of S with the program order determines all of the happens-before edges in the execution. This set is unique."

.link https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.4 JMM ($17.4.5)

* Theory

Executions ‚âà Actions ‚à™ Orders ‚à™ Consistency Rules

Consistency Rules:

: A consistency rule that affects values observed by the actions.

- PO consistency - total order of intra-thread actions. PO is consistent with the source code order in the original program.

: PO consistency affects the structure of the execution.

- SO consistency - covers all synchronization actions. SO is a total order of all SA.
- SO - PO consistency - SW = SO + SO-PO. SW => Coherence (reads see only the latest write in). SW is a partial order.

: Coherence (def.) : The writes to the single memory location appear to be in a total order consistent with program order

- HB consistency - HB is a transitive closure over the union of PO and SW. HB is partial order too.

: HB consistency: reads observe either: the last write in hb order, or any other write, not ordered by hb.

* Theory

- HB + magic => causality
- SO ‚âà Sequential Consistency

: Sequential Consistency (SC): (def.) ¬´...the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program¬ª  Sequential Consistency is not always needed:

Sequential Consistency is not always needed:

- Extreme costs to get it in distributed systems
- Most examples so far were fine with just Release/Acquire!

: see JMM: Example 17.4.8-1. Happens-before Consistency Is Not Sufficient

* Theory

- A program is correctly synchronized if and only if all sequentially consistent executions are free of data races.
- If a program is correctly synchronized, then all executions of the program will appear to be sequentially consistent

* Why?

    int a = 0
    -----------------
    a = 1   |   a = 2
    r1 = a  |

    ----------------------------------------------------

    ùë§(ùëé, 1) ‚àíhb‚àí> ùëü(ùëé) : 1 ... ùë§(ùëé, 2)
    ùë§(ùëé, 1) -hb‚àí> ùëü(ùëé) : 2 ... ùë§(ùëé, 2)

                    [w(a, 1), r(a): 1, w(a, 2)]
                    [w(a, 1), w(a, 2), r(a): 2]

                                            ùëü1 ‚àà {1, 2}

    Original -MM-> {Executions} -yield-> {Outcomes}
    Program                                 ^
      |                                     | (subset of)
      \-impl-> {Impls subset} -yields-> {Results}

                mov 1 ‚Üí (a)                 ùëü1 ‚àà {1}
                mov 1 ‚Üí (r1)

.html mm/source_alex.html

* Go

Is something at [[https://golang.org/ref/mem][golang/mem]] a spec?

* Go

Is something at [[https://golang.org/ref/mem][golang/mem]] a spec?

- No

* What's the point?

Two purposes:

- Make guarantees for programmers.
- Allow compilers/hardware to make certain changes to programs.

Ideally, perfectly balanced. In practice, more conservative: might do neither.

Explicit concern: Leave room for future refinement, refraining from:

- making debatable guarantees to programmers
- allowing compilers/hardware to make debatable changes to programs

.html mm/source_dave.html

* What we are given?

"The Go memory model specifies the conditions under which reads of a variable in one goroutine can be guaranteed to observe values produced by writes to the same variable in a different goroutine."

Nothing about consistency...
or data races

"Programs that modify data being simultaneously accessed by multiple goroutines must serialize such access."

because hardware and compiler may do something...

* How to serialize access?

Use happens-before:

- PO
- HB
- SO, SO-PO

* What Synchronization-With order do we have?

    package q

    func init() {}  <-hb-\
                         |
    ---                  |
                         |
    package p            |
                         |
    import "q"           |
                         |
    func init() {}   ----/


- If p imports q, q‚Äôs init happens before p‚Äôs.
- Package main‚Äôs init happens before main.main

Valid executions:

    [q.init..., p.init...]

* What Synchronization-With order do we have?

    var a string

    func f() {
        print(a)    ---hb---\
    }                       |
                            |
    func hello() {          |
        a = "hello, world"  |
        go f()      <-------/
    }

- The go statement happens before the created goroutine‚Äôs execution

Valid executions:

    [store(a, "hello, world"), read(a): "hello, world"]

* What Synchronization-With order do we have?

    var c = make(chan int, 10)
    var a string

    func f() {
        a = "hello, world" -\
        c <- 0  <---------- | -\
    }                      hb  |
                            |  |
    func main() {           |  |
        go f()  <-----------/  |
        <-c     ----hb---------/
        print(a)
    }

- A send (or close) on a channel happens before the receive

Valid executions:

    [store(a, "..."), send(c, 0), recv(c): 0, read(a): "..."]

* What Synchronization-With order do we have?

    var l sync.Mutex
    var a string

    func f() {
        a = "hello, world" -\
        l.Unlock()  <------ | -\
    }                       |  |
                            |  |
    func main() {           |  |
        l.Lock()            |  |
        go f()  <----hb-----/  |
        l.Lock()    -----hb----/
        print(a)
    }

- Unlock happens before subsequent Lock

Valid executions:

    [lock(l), store(a, "..."), unlock(l), lock(l), read(a): "..."]

* Hmmm...

> Unlock happens before subsequent Lock

* How the Mutex works as a memory barrier?

SW is given in terms of language derivatives.

* How the Mutex is implemented?

    // Fast path: grab unlocked mutex.
    if atomic.CompareAndSwapInt32(&m.state, 0, mutexLocked) {...}

    // Long path:
    runtime_canSpin() -> runtime_doSpin()
    + starvation detection: runtime_nanotime()-waitStartTime > starvationThresholdNs
    + lock fairness with LIFO/Q semaphore runtime_SemacquireMutex(&m.sema, lifo: waitStartTime != 0)
    + race detection

sync/mutex is implemented with sync/atomic only.

* How the Mutex works as a memory barrier?

the sync/atomics works as a memory barriers.

* What about sync/atomic in mm?

?

* What about sync/atomic?

Package sync/atomic is conspicuously missing.

Proposal: match Java volatile and C++ memory_order_seq_cst.

- An atomic write happens before an atomic read that observes the write.
- The outcome of atomic operations must be consistent with a total order over all atomic operations in the program.

.link https://github.com/golang/go/issues/5045 doc: define how sync/atomic interacts with memory model #5045

TODO: examples of atomics litmus tests

* How atomics works as a memory barrier?

on x86/amd64

    TEXT ¬∑CompareAndSwapUint32(SB),NOSPLIT,$0-17
        MOVQ	addr+0(FP), BP
        MOVL	old+8(FP), AX
        MOVL	new+12(FP), CX
        LOCK                    <---------
        CMPXCHGL	CX, 0(BP)
        SETEQ	swapped+16(FP)
        RET

they use LOCK-prefixed instructions like CMPXCHG, XCHGQ, XADDL

* How atomics works as a memory barrier?

on x86/amd64

Section 8.2.5:

    The I/O instructions, locking instructions, the LOCK prefix, and
    serializing instructions force stronger ordering on the processor.

Also from 8.2.5:

    "Like the I/O and locking instructions, the processor waits until
    all previous instructions have been completed and all buffered writes
    have been drained to memory before executing the serializing instruction."

.link https://stackoverflow.com/questions/50280857/do-locked-instructions-provide-a-barrier-between-weakly-ordered-accesses
.link https://peeterjoot.wordpress.com/2009/12/04/intel-memory-ordering-fence-instructions-and-atomic-operations/

* How atomics works as a memory barrier?

on arm64

    TEXT ¬∑CompareAndSwapUint32(SB),NOSPLIT,$0-17
        MOVD	addr+0(FP), R0
        MOVW	old+8(FP), R1
        MOVW	new+12(FP), R2
    again:
        LDAXRW	(R0), R3     <------------ implicit
        CMPW	R1, R3
        BNE	ok
        STLXRW	R2, (R0), R3 <------------ rel/acq mem barrier
        CBNZ	R3, again
    ok:
        CSET	EQ, R0
        MOVB	R0, swapped+16(FP)
        RET

* How atomics works as a memory barrier?

on arm

    TEXT ¬∑armCompareAndSwapUint32(SB),NOSPLIT,$0-13
        ...
    casloop:
        // LDREX and STREX were introduced in ARMv6.
        LDREX	(R1), R0
        CMP	R0, R2
        BNE	casfail
        DMB_ISHST_7
        STREX	R3, (R1), R0
        CMP	$0, R0
        BNE	casloop
        MOVW	$1, R0
        DMB_ISH_7  <--------- explicit memory barrier
        MOVBU	R0, swapped+12(FP)
        RET
    casfail:
        MOVW	$0, R0
        MOVBU	R0, swapped+12(FP)
        RET

* How atomics works as a memory barrier?

on arm

- Load-Acquire (LDAR): All loads and stores that are after an LDAR in program order, and that match the shareability domain of the target address, must be observed after the LDAR.
- Store-Release (STLR): All loads and stores preceding an STLR that match the shareability domain of the target address must be observed before the STLR.
- There are also exclusive versions of the above, LDAXR and STLXR, available.

.link https://developer.arm.com/products/architecture/a-profile/docs/100941/latest/barriers
.link http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/CEGDBEJE.html
.link https://stackoverflow.com/questions/21535058/arm64-ldxr-stxr-vs-ldaxr-stlxr
.link http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/ch06s03s11.html ARM Cortext-A > Multi-processing systems > Synchronization

* Data races

    var a string

    func hello() {
        go func() { a = "hello" }()
        print(a)
    }

> The exit of a goroutine is not guaranteed to happen before any event in the program.

Valid executions:

    [read(a)]
    [read(a), store(a)]
    [store(a), read(a)]

* Data races

.code mm/print_ab.go /func f/,

> it can happen that g prints 2 and then 0.

One of the valid executions:

    [store(b, 2), read(b): 2, read(a): 0, store(a, 1)]

* Data races

.code mm/wait_idiom.go /func setup/,

> there is no guarantee that, in doprint, observing the write to done implies observing the write to a. This version can (incorrectly) print an empty string instead of "hello, world".

One of the valid executions:

    [store(done, true), read(done), read(a): "", store(a, "...")]

* Data races

.code mm/twoprints.go /func setup/,

> As before, there is no guarantee that, in main, observing the write to done implies observing the write to a, so this program could print an empty string too. Worse, there is no guarantee that the write to done will ever be observed by main, since there are no synchronization events between the two threads. The loop in main is not guaranteed to finish.

* Data races

.code mm/unsafe_construct.go /type T/,

> Even if main observes g != nil and exits its loop, there is no guarantee that it will observe the initialized value for g.msg.

* Detection of data races

TODO

* Race detector implementation

TODO

* Links

- "The Go Memory Model", Version of May 31, 2014, https://golang.org/ref/mem
- "Go‚Äôs Memory Model", Russ Cox, rsc@google.com, MIT 6.824 / February 25, 2016
- "Java Memory Model Unlearning Experience", Aleksey Shipil—ëv, shade@redhat.com, @shipilev, 2018
- "Close Encounters of The Java Memory Model Kind", Aleksey Shipil—ëv, 2016
- "Java Memory Model Pragmatics (transcript)", Aleksey Shipil—ëv, 2014
- "C++ Memory Model", https://en.cppreference.com/w/cpp/language/memory_model

* Links

- [[https://github.com/golang/go/issues/5045][doc: define how sync/atomic interacts with memory model #5045]]
- [[https://stackoverflow.com/questions/50280857/do-locked-instructions-provide-a-barrier-between-weakly-ordered-accesses]]
- [[https://peeterjoot.wordpress.com/2009/12/04/intel-memory-ordering-fence-instructions-and-atomic-operations/]]
- [[https://developer.arm.com/products/architecture/a-profile/docs/100941/latest/barriers]]
- [[http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/CEGDBEJE.html]]
- [[https://stackoverflow.com/questions/21535058/arm64-ldxr-stxr-vs-ldaxr-stlxr]]
- [[http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/ch06s03s11.html][ARM Cortext-A > Multi-processing systems > Synchronization]]

* Quotes

TODO: ...
